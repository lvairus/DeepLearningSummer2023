[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/LLM/index.html",
    "href": "posts/LLM/index.html",
    "title": "Large Language Models",
    "section": "",
    "text": "In his article “Large Language Models in Molecular Biology,” Serafim Batzoglou discusses recent advancements in deep learning-based language models and their potential to significantly impact the field of molecular biology. The convergence of LLMs with large-scale genomic and population health data is expected to propel the understanding and modeling of biomolecular systems with a level of accuracy that surpasses human capacity.\n\nLarge Language Models (LLMs):\nLLMs are advanced neural networks capable of generating text that resembles human language. They’re trained with vast amounts of data and learn to predict subsequent words in a sentence based on preceding words, allowing them to understand patterns, relationships, and context within the text.\n\n\nThe Genetic Dogma:\n\nThe central dogma of molecular biology describes the unidirectional flow of genetic information from DNA to RNA to protein. Within the DNA, there are approximately 20,000 genes responsible for synthesizing proteins, which play crucial roles in various biological processes: acting as structural components, enzymes, and facilitating communication within cells.\nThe process of protein synthesis includes transcription, splicing, and translation. Transcription produces messenger RNA (mRNA) by copying a DNA segment, which is then spliced to remove introns and retain exons, forming mature mRNA. Translation decodes the mRNA sequence into amino acids, which are linked to form proteins.\nGene regulation, mediated by transcription factors and other proteins, ensures the timely and appropriate expression of genes within cells. Chromatin structure, consisting of DNA wrapped around histone proteins, plays a role in gene accessibility and regulation; and histone modifications and DNA methylation influence gene expression by changing chromatin structure.\n\n\nGenetic Variation:\nAn individuals’ DNA and environmental influences shapes their biology throughout their lifetime. While humans share over 99.9% identical DNA, our DNA variants account for the heritability of traits, including contributions to health and disease.\nDNA variants are introduced through mutations in the DNA inherited from parents. Most variants are benign, while some may be deleterious or beneficial. Deleterious variants tend to be statistically eliminated from the population over time through natural selection. Essential genes are conserved with little to no mutation across generations due to their crucial role in fitness. Conversely, less significant genes are prone to more mutations as they have minimal fitness consequences and can be passed down without significant impact.\nAdvancements in DNA sequencing technologies have made data collection rapid and cost-effective. Sequencing-based methods can measure various molecular functions, such as gene expression and chromatin structure. Other technologies like mass spectrometry and X-ray crystallography provide insights into protein levels and structures.\nGenome-wide association studies (GWAS) correlate genetic variants with specific phenotypes, providing valuable insights into gene functions and disease mechanisms. However, large language models (LLMs) are expected to surpass traditional association analyses in linking genetic variation to function. The combination of DNA sequencing technologies, data generation capabilities, and LLMs holds promise in advancing our understanding of genetic variation and its impact on molecular mechanisms and human physiology.\n\n\nProminent Language Models in Molecular Biology:\nIn recent years, significant progress has been made in modeling the central dogma of molecular biology, offering insights into gene function and expression. While fully transforming molecular biology into a computational science or engineering human health is still a work in progress, the current momentum suggests that it’s achievable with more data and development. For example, language models like LLMs excel at learning intricate statistical properties of complex sequential data.\nBreakthroughs in different stages of the central dogma exemplify this progress. Methods like SpliceAI accurately predict gene structure by identifying splicing sites, aiding in genetic disease diagnosis. Advances in protein structure prediction, particularly with AlphaFold, have come close to solving the protein folding problem, revolutionizing biological research and drug discovery. Tools like PrimateAI-3D help annotate genetic variants as benign or pathogenic, contributing to disease diagnosis and drug target identification. Language models such as Enformer show promise in predicting gene expression from DNA sequences alone, shedding light on gene regulation. Foundation models like scGPT and Nucleotide Transformer, trained on extensive data, provide valuable insights into single-cell biology and raw DNA sequences, facilitating various downstream applications.\n\nOverall, the progress in modeling the central dogma of molecular biology demonstrates the potential of AI and deep learning in understanding gene function, expression, and regulation. Continued advancements and integration of diverse data sources will enhance our understanding of molecular biology and its impact on medicine and human health.\n\n\nLooking Forward\nThe availability of rich and affordable data is a significant driver of progress in this field. Advances in DNA sequencing technology have significantly reduced the cost of genome sequencing and other molecular assays, enabling comprehensive profiling of gene expression, chromatin structure, and other molecular layers. Initiatives like the UK Biobank and All Of Us project have collected extensive genetic and health data from large cohorts of participants, offering a wealth of information for research purposes. Cancer-focused companies, such as Tempus and Foundation Medicine, are also building vast genomic databases with clinical information.\nWhile developing these technologies, privacy concerns and the role of LLMs in clinical practice must be addressed. Proper informed consent and privacy measures are essential when training LLMs with participant data. LLMs should be seen as tools to assist healthcare professionals rather than replace them. Patient trust and verification remain crucial aspects of their application in clinical settings.\nLLMs are well-suited to integrate and analyze these diverse datasets. Through autoregressive or masked language modeling, these models can learn from single-cell datasets, functional genomics data, and clinical records to understand gene pathways, genomic variants, and their associations with human health. However, the challenge lies in technical innovations to represent and integrate different layers of information and scale up the model’s processing capacity.\nSuch an advanced LLM could have various applications, including clinical diagnosis, drug development, and advancing our understanding of molecular biology. It could aid doctors in making precise diagnoses, identify potential drug targets, and assist in personalized medicine. Additionally, the model could offer suggestions for additional experiments and contribute to filling gaps in data.\n\n\nConclusion\nMolecular biology’s transition into a computational science is facilitated by the combination of extensive data acquisition and powerful models like LLMs. In the coming years, these models will enable accurate predictions of the complex biomolecular interactions that connect our DNA, cellular biology, and overall health. This advancement is expected to have a profound impact on various aspects of medicine. Additionally, the development of open foundation models that integrate genomic and medical data will drive research, innovation, and facilitate the practice of precision medicine."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/python-basics/index.html",
    "href": "posts/python-basics/index.html",
    "title": "Python Basics",
    "section": "",
    "text": "Type\nName\nDescription\nExamples\n\n\n\n\nInteger\nint\nall integers\n-2, -1, 0, 1, 2\n\n\nFloat\nfloat\nall decimals\n1.0, 2.5, 4.987\n\n\nString\nstr\nsequence of characters\n“any text enclosed in single or double quotes, includes spaces and special characters”\n\n\nBoolean\nbool\nexpression that is True or False\nTrue, False, 2==2 (is read as True), 2==1 (is read as False)\n\n\n\nName variables using “camel case” format. Single word variable names should be all lowercase and multi-word names should have all but the first word capitalized capitalize every word after the first and have the rest as lowercase it’s best for variable names to be short but clear so you know what it’s referring to (This is not required but is good practice) example: word, twoWords, varNameExample, variable can contain numbers but cannot start with them Important: variables cannot be named special words that are already define in python, such as any datatype or built-in python function. Python will get confused because the variable name and function name and will give you an error. restricted variable names: int, float, str, bool, if, else, for, while, in, list, dict, def, class"
  },
  {
    "objectID": "posts/python-basics/index.html#operations",
    "href": "posts/python-basics/index.html#operations",
    "title": "Python Basics",
    "section": "Operations:",
    "text": "Operations:\nPython can compute simple calculations with operators\n\n\n\nCharacter\nOperation\nExample\n\n\n\n\n+\naddition\n1 + 2 -&gt; 3\n\n\n-\nsubtraction\n10 - 5 -&gt; 5\n\n\n*\nmultiplication\n3 * 4 -&gt; 12\n\n\n/\ndivision\n15 / 3 -&gt; 5\n\n\n**\npower\n4 ** 2 -&gt; 16\n\n\n//\nfloor division\n7 // 2 -&gt; 3\n\n\n\n(There don’t need to be spaces between the numbers and operations, it’s up to you)"
  },
  {
    "objectID": "posts/python-basics/index.html#user-input",
    "href": "posts/python-basics/index.html#user-input",
    "title": "Python Basics",
    "section": "User Input",
    "text": "User Input\nIf you want to interact with the user of your code, use the input() command\n# General Format: \nuserInput = input(\"message\")\nRunnning this line of code will prompt the user to enter a response based on what you write in the “message.” Their response will then get assigned to your variable, in this case userInput\n# Example\nuserName = input(\"What's your name? \")"
  },
  {
    "objectID": "posts/python-basics/index.html#common-functions",
    "href": "posts/python-basics/index.html#common-functions",
    "title": "Python Basics",
    "section": "Common Functions",
    "text": "Common Functions\nprint(\"text\") # prints out what you write in \"text\"\nlen() # returns the length of a string, list, tuple, or dictionary"
  },
  {
    "objectID": "posts/python-basics/index.html#lists",
    "href": "posts/python-basics/index.html#lists",
    "title": "Python Basics",
    "section": "Lists",
    "text": "Lists\nMaking a list: Lists can be made up of any datatype you can initialize your list with or without elements\n# Empty list\nmyList = []\n\n# List with elements\nnumList = [1, 2, 3, 4, 5]\ncolorList = ['red', 'yellow', 'green', 'blue', 'pink']\nAccessing list values by index: Python numbers each element in order, but it starts from 0\n\n\n\nIndex\n0\n1\n2\n3\n4\n\n\n\n\nnumList\n1\n2\n3\n4\n5\n\n\ncolorList\n‘red’\n‘yellow’\n‘green’\n‘blue’\n‘pink’\n\n\n\n\nthis can get confusing so just keep the 0 index in mind when coding.\n# Accessing elements by index\nlistName[index] # returns the element at the index given\n\n# examples\ncolorList[0] # returns 'red'\ncolorList[4] # returns 'pink'\ncolorList[5] # returns an error because there isn't an index of 5\nThere’s a bunch of other more specific things you can do with lists but you can just look those up as you need while you code"
  },
  {
    "objectID": "posts/python-basics/index.html#if-else-statements",
    "href": "posts/python-basics/index.html#if-else-statements",
    "title": "Python Basics",
    "section": "If Else Statements",
    "text": "If Else Statements\nif condition:\n    # do something\nelse:\n    # do another thing\nyou can also add more possible conditions with elif\nif condition:\n    # do something\nelif other_condition:\n    # do something else\nelif other_condition:\n    # do something else \nelse:\n    # do another thing"
  },
  {
    "objectID": "posts/python-basics/index.html#for-and-while-loops",
    "href": "posts/python-basics/index.html#for-and-while-loops",
    "title": "Python Basics",
    "section": "For and While Loops",
    "text": "For and While Loops\nfor i in range(5):\n    # do something 5 times, \n    # i starts at 0 and ends at 4, increasing by 1 each time\ncolors = ['red', 'yellow', 'green', 'blue', 'pink']\nfor x in numList:\n        # do something with x\n        # x will start at the first index and end at the last index, \n        # going up one index each time\nWhile loops continue until the condition given to them is False\nwhile condition:\n    # do something\n    # something happens that changes the condition to become False\nbe careful not to make an infinite loop. There must always be something in the loop that changes the condition to eventually become False"
  },
  {
    "objectID": "posts/python-basics/index.html#making-functions",
    "href": "posts/python-basics/index.html#making-functions",
    "title": "Python Basics",
    "section": "Making Functions",
    "text": "Making Functions\ndef functionName(parameter):\n        # do something\n        return result\nNote: there doesn’t always have to be a parameter and there doesn’t always have to be a result"
  },
  {
    "objectID": "posts/python-basics/index.html#object-oriented-programming",
    "href": "posts/python-basics/index.html#object-oriented-programming",
    "title": "Python Basics",
    "section": "Object Oriented Programming:",
    "text": "Object Oriented Programming:\nGeneral format:\n# Creating a class\n\nclass ClassName:\n    def __init__(self, parameters):\n        # Constructor\n             self.parameters = parameters   # setting up the variables in the object\n\n    def methodName(self, parameters):\n        # Method\n        return result\n\n# Creating an instance of the class (this is the object)\nobject = ClassName(parameters)\n\n# Calling methods on object\nobject.methodName(parameters)\nself will always be a parameter in all the functions when creating a class because it needs a way of knowing how to refer to itself. however, self does not appear as a parameter outside of creating the class (I think because it’s implied? idk don’t worry about it), so when you create the object and call methods on it, you don’t use self in the parameters.\nExample:\nclass Car:\n    def __init__(self, brand, color):\n        self.brand = brand\n        self.color = color\n      \n    def changeColor(self, newColor)\n        self.color = newColor\n\n    def startEngine(self):\n        print(\"Engine started for\", self.brand)\n\n# Creating objects of the Car class\ncar1 = Car(\"Toyota\", \"Red\")\ncar2 = Car(\"Honda\", \"Blue\")\n\n\n# Accessing attributes of objects\nprint(car1.brand)  # Output: \"Toyota\"\nprint(car1.color)  # Output: \"Red\"\n\nprint(car2.brand)  # Output: \"Honda\"\nprint(car2.color)  # Output: \"Blue\"\n\n# Calling methods on objects\ncar1.changeColor()\ncar1.start_engine()  # Output: Engine started for Toyota\ncar2.start_engine()  # Output: Engine started for Honda"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Laura’s Blog",
    "section": "",
    "text": "Iris Neural Network Training\n\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nLaura Vairus\n\n\n\n\n\n\n  \n\n\n\n\npopulation structure\n\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\nLaura Vairus\n\n\n\n\n\n\n  \n\n\n\n\nPython Basics\n\n\n\n\n\n\n\ncode\n\n\ntraining\n\n\ntutorial\n\n\ncheatsheet\n\n\n\n\nBasic information and commands for python\n\n\n\n\n\n\nJun 8, 2023\n\n\nLaura Vairus\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nLarge Language Models\n\n\n\n\n\nSummary of LLM article by Serafim Batzoglou\n\n\n\n\n\n\nJun 6, 2023\n\n\nLaura Vairus\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2023-06-13-population-structure/index.html",
    "href": "posts/2023-06-13-population-structure/index.html",
    "title": "population structure",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(devtools)\n\nLoading required package: usethis\n\nlibrary(glue)\n\n\nif(!file.exists(glue(\"~/Downloads/analysis_population_structure.tgz\")))\n{\n  system(glue(\"wget -O ~/Downloads/analysis_population_structure.tgz https://uchicago.box.com/shared/static/zv1jyevq01mt130ishx25sgb1agdu8lj.tgz\"))\n  ## tar -xf file_name.tar.gz --directory /target/directory\n}\nsystem(glue(\"tar xvf ~/Downloads/analysis_population_structure.tgz --directory ~/Downloads/\")) \n\nThe following code is loading a function in gist.github.com/38431b74c6c0bf90c12f devtools::source_gist(\"38431b74c6c0bf90c12f\")\n\nwork.dir =\"~/Downloads/analysis_population_structure/\"\n\ndevtools::source_gist(\"38431b74c6c0bf90c12f\")\n\nℹ Sourcing gist \"38431b74c6c0bf90c12f\"\nℹ SHA-1 hash of file is \"cbeca7fd9bf1602dee41c4f1880cc3a5e8992303\"\n\n\n\npopinfo = read_tsv(paste0(work.dir,\"relationships_w_pops_051208.txt\"))\n\nRows: 1301 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): FID, IID, dad, mom, population\ndbl (2): sex, pheno\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\npopinfo %&gt;% count(population)\n\n# A tibble: 11 × 2\n   population     n\n   &lt;chr&gt;      &lt;int&gt;\n 1 ASW           90\n 2 CEU          180\n 3 CHB           90\n 4 CHD          100\n 5 GIH          100\n 6 JPT           91\n 7 LWK          100\n 8 MEX           90\n 9 MKK          180\n10 TSI          100\n11 YRI          180\n\n\n\nsamdata = read_tsv(paste0(work.dir,\"phase3_corrected.psam\"),guess_max = 2500) \n\nRows: 2504 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): #IID, PAT, MAT, SuperPop, Population\ndbl (1): SEX\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspec(samdata)\n\ncols(\n  `#IID` = col_character(),\n  PAT = col_character(),\n  MAT = col_character(),\n  SEX = col_double(),\n  SuperPop = col_character(),\n  Population = col_character()\n)\n\n\n\nsuperpop = samdata %&gt;% select(SuperPop,Population) %&gt;% unique()\nsuperpop = rbind(superpop, data.frame(SuperPop=c(\"EAS\",\"HIS\",\"AFR\"),Population=c(\"CHD\",\"MEX\",\"MKK\")))\n\n\nif(!file.exists(glue::glue(\"{work.dir}output/allhwe.hwe\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --hardy --out {work.dir}output/allhwe\"))\nallhwe = read.table(glue::glue(\"{work.dir}output/allhwe.hwe\"),header=TRUE,as.is=TRUE)\nhist(allhwe$P)\n\n\n\n\n\nqqunif(allhwe$P,main='HWE HapMap3 All Pop')\n\nWarning in qqunif(allhwe$P, main = \"HWE HapMap3 All Pop\"): thresholding p to\n1e-30\n\n\n\n\n\n\npop = \"CHB\"\npop = \"CEU\"\npop = \"YRI\"\nfor(pop in c(\"CHB\",\"CEU\",\"YRI\"))\n{\n  ## what if we calculate with single population?\n  popinfo %&gt;% filter(population==pop) %&gt;%\n    write_tsv(path=glue::glue(\"{work.dir}{pop}.fam\") )\n  if(!file.exists(glue::glue(\"{work.dir}output/hwe-{pop}.hwe\")))\n  system(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --hardy --keep {work.dir}{pop}.fam --out {work.dir}output/hwe-{pop}\"))\n  pophwe = read.table(glue::glue(\"{work.dir}output/hwe-{pop}.hwe\"),header=TRUE,as.is=TRUE)\n  hist(pophwe$P,main=glue::glue(\"HWE {pop} and founders only\"))\n  qqunif(pophwe$P,main=glue::glue(\"HWE {pop} and founders only\"))\n}\n\nWarning: The `path` argument of `write_tsv()` is deprecated as of readr 1.4.0.\nℹ Please use the `file` argument instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nigrowth = read_tsv(\"https://raw.githubusercontent.com/hakyimlab/igrowth/master/rawgrowth.txt\")\n\nRows: 3726 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): IID, pop, serum\ndbl (4): sex, experim, meas.by, growth\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspec(igrowth)\n\ncols(\n  IID = col_character(),\n  sex = col_double(),\n  pop = col_character(),\n  experim = col_double(),\n  meas.by = col_double(),\n  serum = col_character(),\n  growth = col_double()\n)\n\n\n\nigrowth = popinfo %&gt;% select(-pheno) %&gt;% inner_join(igrowth %&gt;% select(IID,growth), by=c(\"IID\"=\"IID\"))\nwrite_tsv(igrowth,path=glue::glue(\"{work.dir}igrowth.pheno\"))\nigrowth %&gt;% ggplot(aes(population,growth)) + geom_violin(aes(fill=population)) + geom_boxplot(width=0.2,col='black',fill='gray',alpha=.8) + theme_bw(base_size = 15)\n\nWarning: Removed 130 rows containing non-finite values (`stat_ydensity()`).\n\n\nWarning: Removed 130 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\nsummary( lm(growth~population,data=igrowth) )\n\n\nCall:\nlm(formula = growth ~ population, data = igrowth)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-58821 -18093  -2242  15896  98760 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    73080.8      938.2  77.894  &lt; 2e-16 ***\npopulationCEU  -2190.1     1175.4  -1.863   0.0625 .  \npopulationCHB   9053.1     2043.9   4.429 9.73e-06 ***\npopulationJPT   3476.8     2034.8   1.709   0.0876 .  \npopulationYRI  -7985.2     1137.2  -7.022 2.61e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24160 on 3591 degrees of freedom\n  (130 observations deleted due to missingness)\nMultiple R-squared:  0.0345,    Adjusted R-squared:  0.03342 \nF-statistic: 32.08 on 4 and 3591 DF,  p-value: &lt; 2.2e-16\n\n\n\nif(!file.exists(glue::glue(\"{work.dir}output/igrowth.assoc.linear\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --linear --pheno {work.dir}igrowth.pheno --pheno-name growth --maf 0.05 --out {work.dir}output/igrowth\"))\nigrowth.assoc = read.table(glue::glue(\"{work.dir}output/igrowth.assoc.linear\"),header=T,as.is=T)\nhist(igrowth.assoc$P)\n\n\n\n\n\nqqunif(igrowth.assoc$P)\n\n\n\n\n\nlibrary(qqman)\n\n\n\n\nFor example usage please run: vignette('qqman')\n\n\n\n\n\nCitation appreciated but not required:\n\n\nTurner, (2018). qqman: an R package for visualizing GWAS results using Q-Q and manhattan plots. Journal of Open Source Software, 3(25), 731, https://doi.org/10.21105/joss.00731.\n\n\n\n\nmanhattan(igrowth.assoc, chr=\"CHR\", bp=\"BP\", snp=\"SNP\", p=\"P\" )\n\n\n\n\n\n## generate PCs using plink\nif(!file.exists(glue::glue(\"{work.dir}output/pca.eigenvec\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --pca --out {work.dir}output/pca\"))\n## read plink calculated PCs\npcplink = read.table(glue::glue(\"{work.dir}output/pca.eigenvec\"),header=F, as.is=T)\nnames(pcplink) = c(\"FID\",\"IID\",paste0(\"PC\", c(1:(ncol(pcplink)-2))) )\npcplink = popinfo %&gt;% left_join(superpop,by=c(\"population\"=\"Population\")) %&gt;% inner_join(pcplink, by=c(\"FID\"=\"FID\", \"IID\"=\"IID\"))\n## plot PC1 vs PC2\npcplink %&gt;% ggplot(aes(PC1,PC2,col=population,shape=SuperPop)) + geom_point(size=3,alpha=.7) + theme_bw(base_size = 15)\n\n\n\n\n\nif(!file.exists(glue::glue(\"{work.dir}output/igrowth-adjPC.assoc.linear\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --linear --pheno {work.dir}igrowth.pheno --pheno-name growth --covar {work.dir}output/pca.eigenvec --covar-number 1-4 --hide-covar --maf 0.05 --out {work.dir}output/igrowth-adjPC\"))\nigrowth.adjusted.assoc = read.table(glue::glue(\"{work.dir}output/igrowth-adjPC.assoc.linear\"),header=T,as.is=T)\n##indadd = igrowth.adjusted.assoc$TEST==\"ADD\"\ntitulo = \"igrowh association adjusted for PCs\"\nhist(igrowth.adjusted.assoc$P,main=titulo)\n\n\n\n\n\nqqunif(igrowth.adjusted.assoc$P,main=titulo)"
  },
  {
    "objectID": "posts/iris-nn/test.html",
    "href": "posts/iris-nn/test.html",
    "title": "Iris Neural Network Training",
    "section": "",
    "text": "# importing packages\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\n\n\n\n# defining NN\n\"\"\"\nclass Net(nn.Module):\n\n    def __init__(self, inp, hl1, hl2, hl3, out):\n        super(Net, self).__init__()\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(inp, hl1) \n        self.fc2 = nn.Linear(hl1, hl2)\n        self.fc4 = nn.Linear(hl2, hl3)\n        self.fc5 = nn.Linear(hl3, out)\n        self.s = nn.Sigmoid()\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc4(x))\n        x = self.fc5(x)\n        x = self.s(x)\n        return x\n\"\"\"\n\nclass Net(nn.Module):\n    def __init__(self, layer_sizes):\n        super(Net, self).__init__()\n        self.layers = nn.ModuleList()\n        self.num_layers = len(layer_sizes) - 1\n\n        # Create hidden layers\n        for i in range(self.num_layers):\n            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n        \n        # output layer\n        self.layers.append(nn.Sigmoid())\n\n    def forward(self, x):\n        for i in range(self.num_layers):\n            x = torch.relu(self.layers[i](x))\n\n        return x\n\n\nlayer_sizes = [4, 30, 20, 10, 3]  # Input size, hidden layer sizes, output size\nnet = Net(layer_sizes)\nnet\n\nNet(\n  (layers): ModuleList(\n    (0): Linear(in_features=4, out_features=30, bias=True)\n    (1): Linear(in_features=30, out_features=20, bias=True)\n    (2): Linear(in_features=20, out_features=10, bias=True)\n    (3): Linear(in_features=10, out_features=3, bias=True)\n    (4): Sigmoid()\n  )\n)\n\n\n\noptimizer = optim.SGD(net.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\n\n\n# defining dataset\n\nclass MyDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __len__(self):\n        L = self.x.shape[0]\n        return L\n\n    def __getitem__(self, i):\n        return (self.x[i, :], self.y[i])\n\n\n# getting data\n\niris = datasets.load_iris() # all data\n\ndata = torch.from_numpy(iris.data).float() # splitting into data tensors (X)\ntarget = torch.from_numpy(iris.target).long() # and target tensors (Y)\n\n# splitting dataset into training and testing groups\ntrainX, testX, trainY, testY = train_test_split(data, target, test_size=0.1, random_state=42)\n# X is data/attributes\n# Y is targets/labels\n\n# making data a Dataset object\ntrainDataset = MyDataset(trainX, trainY)\ntestDataset = MyDataset(testX, testY)\n\n# putting Dataset into Loader\ntrainLoader = DataLoader(trainDataset, batch_size=8, shuffle=True)\ntestLoader = DataLoader(testDataset, batch_size=4, shuffle=True)\n\n\n# training NN\n\nnepochs = 100\nepoch_loss = []\n\nfor epoch in range(nepochs):\n    iter_loss = 0\n    for i, (bX, bY) in enumerate(trainLoader):\n        optimizer.zero_grad()   # zero the gradient buffers\n        output = net(bX)\n        loss = criterion(output, bY)\n        loss.backward()\n        optimizer.step()\n        #print(f'iteration {i}: loss {loss.item()}')\n        iter_loss += loss.item()\n    epoch_loss.append(iter_loss/(i+1))\n    # print(f'epoch {epoch}: loss {iter_loss/(i+1)}')\n\n    # print(loss)\n\n\nepoch_loss\n\n[1.089932595982271,\n 1.0780915232265698,\n 1.0698856536079855,\n 1.0620966939365162,\n 1.051463221802431,\n 1.0407792960896212,\n 1.0263941287994385,\n 1.006874662988326,\n 0.9818673975327435,\n 0.9543773672159981,\n 0.9278924991102779,\n 0.892082112676957,\n 0.8528847378842971,\n 0.8151910129715415,\n 0.7762638295398039,\n 0.7339987334083108,\n 0.6915121253798989,\n 0.6529985280597911,\n 0.6120010228718028,\n 0.577684718019822,\n 0.537588391233893,\n 0.5076155101551729,\n 0.48230401382726784,\n 0.45989446254337535,\n 0.44066135672961965,\n 0.4121406087104012,\n 0.3928568380720475,\n 0.3732422344824847,\n 0.3446122083593817,\n 0.33735772266107444,\n 0.3142062723636627,\n 0.3080035522580147,\n 0.2785126128617455,\n 0.2714738609159694,\n 0.25107884670005126,\n 0.2379030300413861,\n 0.22928375860347466,\n 0.2026286813266137,\n 0.2178823264206157,\n 0.19517485665924408,\n 0.20197849501581752,\n 0.1951888777753886,\n 0.19636180690106222,\n 0.19281140714883804,\n 0.16958910445956624,\n 0.17367594732957728,\n 0.15995553683708696,\n 0.14281793201670928,\n 0.20035550633774085,\n 0.1659274809062481,\n 0.1421836890718516,\n 0.14311273231664123,\n 0.119171392172575,\n 0.14668690128361478,\n 0.16042900331975782,\n 0.1102606740725391,\n 0.1581820836838554,\n 0.13461540573660066,\n 0.16296004821710727,\n 0.1213293199920479,\n 0.12789003748227568,\n 0.16090759580187938,\n 0.1529751457821797,\n 0.10146084499052342,\n 0.14100522230214932,\n 0.1049810720826773,\n 0.11155775633147534,\n 0.10951150008751188,\n 0.12753714467672742,\n 0.17456571779706898,\n 0.10472052445744767,\n 0.09768917438957621,\n 0.10721259117674302,\n 0.10751658004215535,\n 0.11949179442051579,\n 0.13707306488033602,\n 0.13714948592378812,\n 0.1302530608304283,\n 0.1622128228602164,\n 0.1422118198893526,\n 0.10186021829790928,\n 0.10798571625834003,\n 0.0845348046928206,\n 0.21633452007218318,\n 0.13113987621139078,\n 0.13525098699199803,\n 0.11727344157064662,\n 0.138534656534081,\n 0.09484674929476836,\n 0.11408404238960322,\n 0.08664089983657879,\n 0.09082044567912817,\n 0.08115927672342342,\n 0.1474215361454031,\n 0.15857262180789428,\n 0.12826845986659036,\n 0.11472896952182055,\n 0.2042937748517622,\n 0.09265142229988295,\n 0.09727475545643006]\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(epoch_loss)\n\n\n\n\n\n# testing NN\n\nprint(\"testing\")\noptimizer.zero_grad()\noutput = net(testX)\nprint(testX.shape)\nloss = criterion(output, testY)\nprint(loss)\n# print(output)\n\ntesting\ntorch.Size([15, 4])\ntensor(0.1146, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n# analyzing and printing results\n\npreds = []\nfor row in output:\n    if row.max() == row[0]:\n        preds.append(0)\n    elif row.max() == row[1]:\n        preds.append(1)\n    else:\n        preds.append(2)\ntPreds = torch.tensor(preds).view(15,1)\ntTargets = testY.view(15,1)\n\nresult = torch.cat([tPreds,tTargets], dim=1)\nprint(result)\ncorrect = 0\nfor row in result:\n    if row[0] == row[1]:\n        correct += 1\n\nprint(correct)\n\ntensor([[1, 1],\n        [0, 0],\n        [2, 2],\n        [1, 1],\n        [1, 1],\n        [0, 0],\n        [1, 1],\n        [2, 2],\n        [2, 1],\n        [1, 1],\n        [2, 2],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0]])\n14"
  }
]