{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Enformer Reference Epigenome\"\n",
    "author: \"Laura Vairus\"\n",
    "date: \"2023-08-10\"\n",
    "categories: [howto, code]\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "L = 114688\n",
    "L2 = L // 2\n",
    "L4 = L // 4\n",
    "\n",
    "num_tracks = 5313\n",
    "num_bins = 896\n",
    "\n",
    "Q = num_bins // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chromosome lenghts\n",
    "chr1_len = 248956422\n",
    "chr2_len = 242193529\n",
    "chr3_len = 198295559\n",
    "chr4_len = 190214555\n",
    "chr5_len = 181538259\n",
    "chr6_len = 170805979\n",
    "chr7_len = 159345973\n",
    "chr8_len = 145138636\n",
    "chr9_len = 138394717\n",
    "chr10_len = 133797422\n",
    "chr11_len = 135086622\n",
    "chr12_len = 133275309\n",
    "chr13_len = 114364328\n",
    "chr14_len = 107043718\n",
    "chr15_len = 101991189\n",
    "chr16_len = 90338345\n",
    "chr17_len = 83257441\n",
    "chr18_len = 80373285\n",
    "chr19_len = 58617616\n",
    "chr20_len = 64444167\n",
    "chr21_len = 46709983\n",
    "chr22_len = 50818468\n",
    "chrX_len = 156040895\n",
    "chrY_len = 57227415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction folder paths\n",
    "\n",
    "chr1_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr1_reference_overlapping_regions/predictions_2023-07-21/chr1_predictions/chr1_reference/haplotype0\"\n",
    "chr2_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr2_reference_overlapping_regions/predictions_2023-07-21/chr2_predictions/chr2_reference/haplotype0\"\n",
    "chr3_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr3_reference_overlapping_regions/predictions_2023-08-07/chr3_predictions/chr3_reference/haplotype0\"\n",
    "chr4_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr4_reference_overlapping_regions/predictions_2023-08-07/chr4_predictions/chr4_reference/haplotype0\"\n",
    "chr5_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr5_reference_overlapping_regions/predictions_2023-08-07/chr5_predictions/chr5_reference/haplotype0\"\n",
    "chr6_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr6_reference_overlapping_regions/predictions_2023-08-07/chr6_predictions/chr6_reference/haplotype0\"\n",
    "chr7_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr7_reference_overlapping_regions/predictions_2023-08-07/chr7_predictions/chr7_reference/haplotype0\"\n",
    "chr8_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr8_reference_overlapping_regions/predictions_2023-07-21/enformer_predictions/reference_enformer/haplotype0\"\n",
    "chr9_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr9_reference_overlapping_regions/predictions_2023-07-21/chr9_predictions/chr9_reference/haplotype0\"\n",
    "chr10_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr10_reference_overlapping_regions/predictions_2023-07-21/chr10_predictions/chr10_reference/haplotype0\"\n",
    "chr11_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr11_reference_overlapping_regions/predictions_2023-07-21/chr11_predictions/chr11_reference/haplotype0\"\n",
    "chr12_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr12_reference_overlapping_regions/predictions_2023-07-21/chr12_predictions/chr12_reference/haplotype0\"\n",
    "chr13_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr13_reference_overlapping_regions/predictions_2023-07-21/chr13_predictions/chr13_reference/haplotype0\"\n",
    "chr14_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr14_reference_overlapping_regions/predictions_2023-08-07/chr14_predictions/chr14_reference/haplotype0\"\n",
    "chr15_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr15_reference_overlapping_regions/predictions_2023-08-07/chr15_predictions/chr15_reference/haplotype0\"\n",
    "chr16_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr16_reference_overlapping_regions/predictions_2023-08-07/chr16_predictions/chr16_reference/haplotype0\"\n",
    "chr17_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr17_reference_overlapping_regions/predictions_2023-08-07/chr17_predictions/chr17_reference/haplotype0\"\n",
    "chr18_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr18_reference_overlapping_regions/predictions_2023-08-07/chr18_predictions/chr18_reference/haplotype0\"\n",
    "chr19_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr19_reference_overlapping_regions/predictions_2023-08-08/chr19_predictions/chr19_reference/haplotype0\"\n",
    "chr20_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr20_reference_overlapping_regions/predictions_2023-07-21/chr20_predictions/reftile_prediction_data/haplotype0\"\n",
    "chr21_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr21_reference_overlapping_regions/predictions_2023-07-21/chr21_predictions/reftile_prediction_data/haplotype0\"\n",
    "chr22_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr22_reference_overlapping_regions/predictions_2023-07-25/chr22_predictions/chr22_prediction_data/haplotype0\"\n",
    "chrX_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chrX_reference_overlapping_regions/predictions_2023-07-25/chrX_predictions/chrX_prediction_data/haplotype0\"\n",
    "chrY_pred_path = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chrY_reference_overlapping_regions/predictions_2023-07-25/chrY_predictions/chrY_reference/haplotype0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chr_ints_list(chr_num, chr_len,):\n",
    "    intervals = []\n",
    "    \n",
    "    # determine number of tiles\n",
    "    num_tiles = chr_len // L2\n",
    "    if L2*(num_tiles)+L4 < chr_len:\n",
    "        num_tiles += 1\n",
    "\n",
    "    # create intervals\n",
    "    for i in range(num_tiles):\n",
    "        start = L2 * i\n",
    "        end = L2 * (i+2)\n",
    "        interval = f'chr{chr_num}_{start}_{end}'\n",
    "        intervals.append(interval)\n",
    "            \n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_predictions(predictions_folder, concat_result_file, chr_num, chr_len):\n",
    "    intervals = get_chr_ints_list(chr_num, chr_len)\n",
    "    dest_data_name = f'chr{chr_num}'\n",
    "\n",
    "    # create empty dataset\n",
    "    with h5py.File(concat_result_file, \"w\") as dest_file: # Create or open the new big HDF5 file in write mode\n",
    "        last_interval_start = int(intervals[-1].split('_')[1])\n",
    "        num_last_bins = ((chr_len-(last_interval_start+L4)) // 128) + 1\n",
    "        data_length = (Q*3) + ((len(intervals)-2)*Q*2) + num_last_bins # Calculate the total length of the dataset\n",
    "        dest_file.create_dataset(name=dest_data_name, fillvalue=np.nan, shape=(data_length, 5313), chunks=(1000, 5313))\n",
    "        \n",
    "        # try removing chunks\n",
    "\n",
    "    # append to dataset\n",
    "    with h5py.File(concat_result_file, \"a\") as dest_file:\n",
    "\n",
    "        # first tile\n",
    "        first_tile = f'{predictions_folder}/{intervals[0]}_predictions.h5'\n",
    "        if os.path.exists(first_tile):\n",
    "            with h5py.File(source_file, \"r\") as f: # Open first tile HDF5 file in read mode\n",
    "                tile_data = f[intervals[0]][()] # Access the dataset within the group\n",
    "                data_to_concat = tile_data[:Q*3] # Extract the first 3/4ths of the dataset\n",
    "        else:\n",
    "            data_to_concat = np.full((Q*2, 5313), np.nan) # first 2 Qs are nan\n",
    "            second_tile = f'{predictions_folder}/{intervals[1]}_predictions.h5'\n",
    "            if os.path.exists(second_tile):\n",
    "                with h5py.File(second_tile, \"r\") as f: # Open first tile HDF5 file in read mode\n",
    "                    tile_data = f[intervals[1]][()] # Access the np array within the group\n",
    "                    next_tileQ = tile_data[:Q] # Extract first Q\n",
    "                    data_to_concat.append(next_tileQ) # 3rd Q is from second tile\n",
    "            else:\n",
    "                data_to_concat = np.full((Q*3, 5313), np.nan) # all 3 Qs are nan\n",
    "\n",
    "        dest_file[dest_data_name][:Q*3] = data_to_concat # add to dataset\n",
    "\n",
    "        # middle tiles\n",
    "        for i in range(1, len(intervals)-1):\n",
    "            curr_tile = f'{predictions_folder}/{intervals[i]}_predictions.h5'\n",
    "            if os.path.exists(curr_tile):        \n",
    "                with h5py.File(source_file, \"r\") as f: # Open each source HDF5 file in read mode\n",
    "                    tile_data = f[intervals[i]][()] # Access the dataset within the group\n",
    "                    data_to_concat = tile_data[Q:Q*3] # Extract the middle section of the dataset\n",
    "            else:\n",
    "                prev_tile = f'{predictions_folder}/{intervals[i-1]}_predictions.h5'\n",
    "                next_tile = f'{predictions_folder}/{intervals[i+1]}_predictions.h5'\n",
    "\n",
    "                if os.path.exists(prev_tile): # if prev tile exists\n",
    "                    with h5py.File(prev_tile, \"r\") as f:\n",
    "                        tile_data = f[intervals[i-1]][()]\n",
    "                        data_to_concat = tile_data[-Q:] # data starts with last Q from prev tile\n",
    "                else:\n",
    "                    data_to_concat = np.full((Q, 5313), np.nan) # datas first Q is nan\n",
    "                \n",
    "                if os.path.exists(next_tile): # if next tile exists\n",
    "                    with h5py.File(next_tile, \"r\") as f:\n",
    "                        tile_data = f[intervals[i+1]][()]\n",
    "                        data_to_concat.append(tile_data[:Q]) # data ends with first Q from next tile\n",
    "                else:\n",
    "                    data_to_concat.append(np.full((Q, 5313), np.nan)) # datas last Q is nan\n",
    "            \n",
    "            dest_file[dest_data_name][Q*(1+2*i):Q*(3+2*i)] = data_to_concat\n",
    "        \n",
    "\n",
    "        # last tile\n",
    "        last_tile = f'{predictions_folder}/{intervals[-1]}_predictions.h5'\n",
    "        if os.path.exists(last_tile):\n",
    "            with h5py.File(last_tile, \"r\") as f:\n",
    "                tile_data = f[intervals[-1]][()]\n",
    "                data_to_concat = tile_data[Q:Q+num_last_bins]\n",
    "        else:\n",
    "            prev_tile = f'{predictions_folder}/{intervals[-2]}_predictions.h5'\n",
    "            if os.path.exists(prev_tile):\n",
    "                with h5py.File(prev_tile, \"r\") as f:\n",
    "                    tile_data = f[intervals[-2]][()]\n",
    "                    if num_last_bins > Q:\n",
    "                        data_to_concat = tile_data[-Q:] # data starts with last Q from prev tile \n",
    "                        data_to_concat.append(np.full((num_last_bins-Q, 5313), np.nan)) # ends with nan\n",
    "                    else:\n",
    "                        data_to_concat = tile_data[Q*3:(Q*3)+num_last_bins] # data ends with prev tile 3Q to end of chrom\n",
    "            else:\n",
    "                data_to_concat = np.full((num_last_bins, 5313), np.nan) # all last bins are nan\n",
    "        dest_file[dest_data_name][-num_last_bins:] = data_to_concat\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_preds(result_file, indiv_folder_path):\n",
    "    \n",
    "    # create empty dataset\n",
    "    with h5py.File(result_file, \"w\") as file: \n",
    "        file.create_dataset(name=\"all_predictions\", fillvalue=np.nan, shape=(20000, 600, 500)) # shape=(regions, tracks, people)\n",
    "\n",
    "    # add to dataset\n",
    "    with h5py.File(result_file, \"a\") as file: \n",
    "\n",
    "        for i, indiv in enumerate(os.listdir(indiv_folder_path)): # loop through every individual\n",
    "            preds_folder_path = f\"{indiv_folder_path}/{indiv}\" # get correct path to predictions folder\n",
    "            for p, pred in enumerate(os.listdir(preds_folder_path)): # loop through every h5 file\n",
    "                pred_path = f\"{preds_folder_path}/{pred}\" # get correct path to h5 file\n",
    "                with h5py.File(pred_path, \"r\") as f: # read in prediction data\n",
    "                    dataset_name = \"idk find out\"\n",
    "                    data = f[dataset_name][()] \n",
    "                data = data.mean() # get avg of the 4 rows\n",
    "\n",
    "                file[\"all_predictions\"][p,:,i] = data\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
