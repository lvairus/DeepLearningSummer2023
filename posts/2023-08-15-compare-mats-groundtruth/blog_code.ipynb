{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Comparing different prediction matrices to ground truth\"\n",
    "author: \"Laura Vairus\"\n",
    "date: \"2023-08-16\"\n",
    "categories: [method, code]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making the epigenome to query, we want it to be as accurate as possible. Some papers decribe they got their predictions by running enformer on the target interval, randomly shifted up and down, reverse complementing, and averaging them all together. To check which way is best, I made 4 different prediction matrices to evaluate by comparing against the ground truth\n",
    "- (forward) normal forward prediction on the target interval\n",
    "- (reverse) reverse complement prediction on the target interval\n",
    "- (forrev) average of the forward and reverse complement predictions above\n",
    "- (forrevshift) average of the forward and reverse complement prediction on the target interval normally, shifted up 3bps, and shifted down 3bps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I had to get the ground truth data. I used the matrices from \"/grand/TFXcan/imlab/data/enformer_training_data/basenji_data_h5/no_groups_popseq_revised_order/test_pop_seq.hdf5\", which were used to test the training of enformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hdf5 file contains 4 datasets: pop_sequence, query_regions, sequence, and target.\n",
    "\n",
    "- query regions has 1937 lists of interval data: chromosome number, start position, and end position. shape (1937, 3)\n",
    "- sequence has 1937 DNA sequeces that are 131072 bps long and encoded into a one-hot-matrix of 4 groups. shape (1947, 131072, 4)\n",
    "- target has 1937 target enformer outputs of the usual (896, 5313) bin by track matrices. shape (1937, 896, 5313)\n",
    "\n",
    "I will use the target matrices as the ground truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was some confusion around whether the 1937 'target' predictions and 'query_region' intervals lined up so I wrote some code to compare each 'sequence' entry to the one-hot-encoded extraction of the corresponding 'query_regions' interval from the hg38 reference genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 19:30:26.117380: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 19:30:30.339908: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.8'; dlerror: libnvinfer.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/nvidia/hpc_sdk/Linux_x86_64/21.9/comm_libs/nvshmeme/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/comm_libs/nccl/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/math_libs/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/compilers/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/compilersextras/qd/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/cudaextras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/cuda/lib64:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.125/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32\n",
      "2023-08-16 19:30:30.340195: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.8'; dlerror: libnvinfer_plugin.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/nvidia/hpc_sdk/Linux_x86_64/21.9/comm_libs/nvshmeme/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/comm_libs/nccl/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/math_libs/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/compilers/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/compilersextras/qd/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/cudaextras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.9/cuda/lib64:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.125/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32\n",
      "2023-08-16 19:30:30.340213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import joblib\n",
    "import gzip\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @title `Enformer`, `EnformerScoreVariantsNormalized`, `EnformerScoreVariantsPCANormalized`,\n",
    "# SEQUENCE_LENGTH = 393216\n",
    "\n",
    "# class Enformer:\n",
    "\n",
    "#   def __init__(self, tfhub_url):\n",
    "#     self._model = hub.load(tfhub_url).model\n",
    "\n",
    "#   def predict_on_batch(self, inputs):\n",
    "#     predictions = self._model.predict_on_batch(inputs)\n",
    "#     return {k: v.numpy() for k, v in predictions.items()}\n",
    "\n",
    "#   @tf.function\n",
    "#   def contribution_input_grad(self, input_sequence,\n",
    "#                               target_mask, output_head='human'):\n",
    "#     input_sequence = input_sequence[tf.newaxis]\n",
    "\n",
    "#     target_mask_mass = tf.reduce_sum(target_mask)\n",
    "#     with tf.GradientTape() as tape:\n",
    "#       tape.watch(input_sequence)\n",
    "#       prediction = tf.reduce_sum(\n",
    "#           target_mask[tf.newaxis] *\n",
    "#           self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n",
    "\n",
    "#     input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n",
    "#     input_grad = tf.squeeze(input_grad, axis=0)\n",
    "#     return tf.reduce_sum(input_grad, axis=-1)\n",
    "\n",
    "\n",
    "# class EnformerScoreVariantsRaw:\n",
    "\n",
    "#   def __init__(self, tfhub_url, organism='human'):\n",
    "#     self._model = Enformer(tfhub_url)\n",
    "#     self._organism = organism\n",
    "\n",
    "#   def predict_on_batch(self, inputs):\n",
    "#     ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]\n",
    "#     alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]\n",
    "\n",
    "#     return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n",
    "\n",
    "\n",
    "# class EnformerScoreVariantsNormalized:\n",
    "\n",
    "#   def __init__(self, tfhub_url, transform_pkl_path,\n",
    "#                organism='human'):\n",
    "#     assert organism == 'human', 'Transforms only compatible with organism=human'\n",
    "#     self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n",
    "#     with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n",
    "#       transform_pipeline = joblib.load(f)\n",
    "#     self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n",
    "\n",
    "#   def predict_on_batch(self, inputs):\n",
    "#     scores = self._model.predict_on_batch(inputs)\n",
    "#     return self._transform.transform(scores)\n",
    "\n",
    "\n",
    "# class EnformerScoreVariantsPCANormalized:\n",
    "\n",
    "#   def __init__(self, tfhub_url, transform_pkl_path,\n",
    "#                organism='human', num_top_features=500):\n",
    "#     self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n",
    "#     with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n",
    "#       self._transform = joblib.load(f)\n",
    "#     self._num_top_features = num_top_features\n",
    "\n",
    "#   def predict_on_batch(self, inputs):\n",
    "#     scores = self._model.predict_on_batch(inputs)\n",
    "#     return self._transform.transform(scores)[:, :self._num_top_features]\n",
    "\n",
    "\n",
    "# # TODO(avsec): Add feature description: Either PCX, or full names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title `variant_centered_sequences`\n",
    "\n",
    "class FastaStringExtractor:\n",
    "\n",
    "    def __init__(self, fasta_file):\n",
    "        self.fasta = pyfaidx.Fasta(fasta_file)\n",
    "        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n",
    "\n",
    "    def extract(self, interval: Interval, **kwargs) -> str:\n",
    "        # Truncate interval if it extends beyond the chromosome lengths.\n",
    "        chromosome_length = self._chromosome_sizes[interval.chrom]\n",
    "        trimmed_interval = Interval(interval.chrom,\n",
    "                                    max(interval.start, 0),\n",
    "                                    min(interval.end, chromosome_length),\n",
    "                                    )\n",
    "        # pyfaidx wants a 1-based interval\n",
    "        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n",
    "                                          trimmed_interval.start + 1,\n",
    "                                          trimmed_interval.stop).seq).upper()\n",
    "        # Fill truncated values with N's.\n",
    "        pad_upstream = 'N' * max(-interval.start, 0)\n",
    "        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n",
    "        return pad_upstream + sequence + pad_downstream\n",
    "\n",
    "    def close(self):\n",
    "        return self.fasta.close()\n",
    "\n",
    "\n",
    "def variant_generator(vcf_file, gzipped=False):\n",
    "  \"\"\"Yields a kipoiseq.dataclasses.Variant for each row in VCF file.\"\"\"\n",
    "  def _open(file):\n",
    "    return gzip.open(vcf_file, 'rt') if gzipped else open(vcf_file)\n",
    "\n",
    "  with _open(vcf_file) as f:\n",
    "    for line in f:\n",
    "      if line.startswith('#'):\n",
    "        continue\n",
    "      chrom, pos, id, ref, alt_list = line.split('\\t')[:5]\n",
    "      # Split ALT alleles and return individual variants as output.\n",
    "      for alt in alt_list.split(','):\n",
    "        yield kipoiseq.dataclasses.Variant(chrom=chrom, pos=pos,\n",
    "                                           ref=ref, alt=alt, id=id)\n",
    "\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n",
    "\n",
    "\n",
    "def variant_centered_sequences(vcf_file, sequence_length, gzipped=False,\n",
    "                               chr_prefix=''):\n",
    "  seq_extractor = kipoiseq.extractors.VariantSeqExtractor(\n",
    "    reference_sequence=FastaStringExtractor(fasta_file))\n",
    "\n",
    "  for variant in variant_generator(vcf_file, gzipped=gzipped):\n",
    "    interval = Interval(chr_prefix + variant.chrom,\n",
    "                        variant.pos, variant.pos)\n",
    "    interval = interval.resize(sequence_length)\n",
    "    center = interval.center() - interval.start\n",
    "\n",
    "    reference = seq_extractor.extract(interval, [], anchor=center)\n",
    "    alternate = seq_extractor.extract(interval, [variant], anchor=center)\n",
    "\n",
    "    yield {'inputs': {'ref': one_hot_encode(reference),\n",
    "                      'alt': one_hot_encode(alternate)},\n",
    "           'metadata': {'chrom': chr_prefix + variant.chrom,\n",
    "                        'pos': variant.pos,\n",
    "                        'id': variant.id,\n",
    "                        'ref': variant.ref,\n",
    "                        'alt': variant.alt}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_extractor38 = FastaStringExtractor('/lus/grand/projects/TFXcan/imlab/data/hg_sequences/hg38/Homo_sapiens_assembly38.fasta')\n",
    "fasta_extractor19 = FastaStringExtractor('/lus/grand/projects/TFXcan/imlab/data/hg_sequences/hg19/raw/genome.fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(f'/grand/TFXcan/imlab/data/enformer_training_data/basenji_data_h5/no_groups_popseq_revised_order/test_pop_seq.hdf5') as f:\n",
    "    intervals = f['query_regions'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       0, 39808995, 39940067])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervals[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_int_seqs(intervals, i):\n",
    "    for interval in intervals:\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(f'now on index {i}, interval {interval}')\n",
    "\n",
    "        chr = interval[0]\n",
    "        start = interval[1]\n",
    "        end = interval[2]\n",
    "\n",
    "        if chr == 0:\n",
    "            chr = 'X'\n",
    "\n",
    "        with h5py.File(f'/grand/TFXcan/imlab/data/enformer_training_data/basenji_data_h5/no_groups_popseq_revised_order/test_pop_seq.hdf5') as f:\n",
    "            target_seq = f['sequence'][()][i,:,:]\n",
    "        \n",
    "        target_interval = kipoiseq.Interval(f'chr{chr}', start, end)\n",
    "        seq38 = one_hot_encode(fasta_extractor38.extract(target_interval))\n",
    "        \n",
    "        if np.array_equal(target_seq, seq38) == False:\n",
    "            print(f'not equal at index {i}, interval {interval}')\n",
    "        \n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now on index 1160, interval [        0 125746319 125877391]\n",
      "now on index 1180, interval [        0 122535027 122666099]\n",
      "now on index 1200, interval [      14 77070902 77201974]\n",
      "now on index 1220, interval [      14 58376595 58507667]\n",
      "now on index 1240, interval [        3 189435603 189566675]\n",
      "now on index 1260, interval [      15 23391563 23522635]\n",
      "now on index 1280, interval [      12 11085476 11216548]\n",
      "now on index 1300, interval [      14 96797410 96928482]\n",
      "now on index 1320, interval [      14 58605973 58737045]\n",
      "now on index 1340, interval [       14 105169707 105300779]\n",
      "now on index 1360, interval [      14 36585685 36716757]\n",
      "now on index 1380, interval [      14 87507601 87638673]\n",
      "now on index 1400, interval [      10 37784915 37915987]\n",
      "now on index 1420, interval [       14 100926214 101057286]\n",
      "now on index 1440, interval [      14 45416738 45547810]\n",
      "now on index 1460, interval [      11 32207325 32338397]\n",
      "now on index 1480, interval [       14 104825640 104956712]\n",
      "now on index 1500, interval [      14 81773151 81904223]\n",
      "now on index 1520, interval [      14 91292338 91423410]\n",
      "now on index 1540, interval [      14 94388941 94520013]\n",
      "now on index 1560, interval [      14 49774920 49905992]\n",
      "now on index 1580, interval [        3 194825986 194957058]\n",
      "now on index 1600, interval [      19 43297334 43428406]\n",
      "now on index 1620, interval [      14 56885638 57016710]\n",
      "now on index 1640, interval [        0 128154788 128285860]\n",
      "now on index 1660, interval [      14 28328077 28459149]\n",
      "now on index 1680, interval [      15 22907247 23038319]\n",
      "now on index 1700, interval [      14 66978270 67109342]\n",
      "now on index 1720, interval [        2 124046842 124177914]\n",
      "now on index 1740, interval [        2 123014641 123145713]\n",
      "now on index 1760, interval [      14 50348365 50479437]\n",
      "now on index 1780, interval [      15 24882520 25013592]\n",
      "now on index 1800, interval [        0 140770578 140901650]\n",
      "now on index 1820, interval [      13 86482357 86613429]\n",
      "now on index 1840, interval [        3 193335029 193466101]\n",
      "now on index 1860, interval [      14 64111045 64242117]\n",
      "now on index 1880, interval [       12 132797807 132928879]\n",
      "now on index 1900, interval [      11 35074550 35205622]\n",
      "now on index 1920, interval [        3 183471775 183602847]\n"
     ]
    }
   ],
   "source": [
    "check_int_seqs(intervals[541:], 541)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every sequence except for the ones on indices 317, 1159, and 541 matched up with the correspomding extracted sequence. the 317 and 1159 sequences gave a KeyError that suggested there might be an \"M\" in the extracted region, and the 514th sequence simply did not match it's correspomdong extraction, both of which should be looked into further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'M'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m target_interval \u001b[39m=\u001b[39m kipoiseq\u001b[39m.\u001b[39mInterval(\u001b[39m'\u001b[39m\u001b[39mchr1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m248654924\u001b[39m, \u001b[39m248785996\u001b[39m)\n\u001b[1;32m      2\u001b[0m extract \u001b[39m=\u001b[39m fasta_extractor38\u001b[39m.\u001b[39mextract(target_interval)\n\u001b[0;32m----> 3\u001b[0m seq38 \u001b[39m=\u001b[39m one_hot_encode(extract)\n",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m, in \u001b[0;36mone_hot_encode\u001b[0;34m(sequence)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mone_hot_encode\u001b[39m(sequence):\n\u001b[0;32m---> 46\u001b[0m   \u001b[39mreturn\u001b[39;00m kipoiseq\u001b[39m.\u001b[39;49mtransforms\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mone_hot_dna(sequence)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/.local/polaris/conda/2023-01-10-unstable/lib/python3.10/site-packages/kipoiseq/transforms/functional.py:135\u001b[0m, in \u001b[0;36mone_hot_dna\u001b[0;34m(seq, dtype)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mastype(dtype)\n\u001b[1;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m one_hot(seq, alphabet\u001b[39m=\u001b[39;49mDNA, neutral_alphabet\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mN\u001b[39;49m\u001b[39m'\u001b[39;49m], neutral_value\u001b[39m=\u001b[39;49m\u001b[39m.25\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[0;32m~/.local/polaris/conda/2023-01-10-unstable/lib/python3.10/site-packages/kipoiseq/transforms/functional.py:120\u001b[0m, in \u001b[0;36mone_hot\u001b[0;34m(seq, alphabet, neutral_alphabet, neutral_value, dtype)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(seq, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mseq needs to be a string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m token2one_hot(tokenize(seq, alphabet, neutral_alphabet), \u001b[39mlen\u001b[39m(alphabet), neutral_value, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/polaris/conda/2023-01-10-unstable/lib/python3.10/site-packages/kipoiseq/transforms/functional.py:101\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(seq, alphabet, neutral_alphabet)\u001b[0m\n\u001b[1;32m     99\u001b[0m     alphabet_dict[l] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[39m# current performance bottleneck\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([alphabet_dict[seq[(i \u001b[39m*\u001b[39m nchar):((i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m nchar)]] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(seq) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m nchar)])\n",
      "File \u001b[0;32m~/.local/polaris/conda/2023-01-10-unstable/lib/python3.10/site-packages/kipoiseq/transforms/functional.py:101\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     99\u001b[0m     alphabet_dict[l] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[39m# current performance bottleneck\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([alphabet_dict[seq[(i \u001b[39m*\u001b[39;49m nchar):((i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m nchar)]] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(seq) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m nchar)])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'M'"
     ]
    }
   ],
   "source": [
    "# \"M\" error output:\n",
    "\n",
    "# 'M' error at index 317, interval ('chr1', 248654924, 248785996)\n",
    "target_interval = kipoiseq.Interval('chr1', 248654924, 248785996)\n",
    "extract = fasta_extractor38.extract(target_interval)\n",
    "seq38 = one_hot_encode(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'M' error at index 1159, interval [12 132224362 132355434]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not equal at 541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing matrices to ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for my first test, I chose the 1000th index from the basenji dataset. The interval was \"chr17_20693681_20824753\". So I put that interval normally, shifted up 3 bps, and shifted down 3bps, and selected the option for computing the reverse complement in the enformer pipeline. After getting the prediction results, I read them all in and calculated the averages I wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in predictions\n",
    "\n",
    "with h5py.File(\"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr17_reference_avg6_shifted_regions/predictions_2023-08-15/chr17_predictions/chr17_reference_avg6/haplotype0/chr17_20693681_20824753_predictions.h5\") as f:\n",
    "    forward = f['chr17_20693681_20824753'][()]\n",
    "\n",
    "with h5py.File(\"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr17_reference_avg6_shifted_regions/predictions_2023-08-15/chr17_predictions/chr17_reference_avg6/haplotype0_rc/chr17_20693681_20824753_predictions.h5\") as f:\n",
    "    reverse = f['chr17_20693681_20824753'][()]\n",
    "\n",
    "forrev = (forward + reverse) / 2\n",
    "\n",
    "with h5py.File(\"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr17_reference_avg6_shifted_regions/predictions_2023-08-15/chr17_predictions/chr17_reference_avg6/haplotype0/chr17_20693678_20824750_predictions.h5\") as f:\n",
    "    shift_down = f['chr17_20693678_20824750'][()]\n",
    "\n",
    "with h5py.File(\"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr17_reference_avg6_shifted_regions/predictions_2023-08-15/chr17_predictions/chr17_reference_avg6/haplotype0_rc/chr17_20693678_20824750_predictions.h5\") as f:\n",
    "    shift_down_rc = f['chr17_20693678_20824750'][()]\n",
    "\n",
    "with h5py.File(\"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr17_reference_avg6_shifted_regions/predictions_2023-08-15/chr17_predictions/chr17_reference_avg6/haplotype0/chr17_20693684_20824756_predictions.h5\") as f:\n",
    "    shift_up = f['chr17_20693684_20824756'][()]\n",
    "\n",
    "with h5py.File(\"/grand/TFXcan/imlab/users/lvairus/reftile_project/predictions_folder/chr17_reference_avg6_shifted_regions/predictions_2023-08-15/chr17_predictions/chr17_reference_avg6/haplotype0_rc/chr17_20693684_20824756_predictions.h5\") as f:\n",
    "    shift_up_rc = f['chr17_20693684_20824756'][()]\n",
    "\n",
    "forrevshift = (forward + reverse + shift_down + shift_down_rc + shift_up + shift_up_rc) / 6\n",
    "\n",
    "with h5py.File(\"/grand/TFXcan/imlab/data/enformer_training_data/basenji_data_h5/no_groups_popseq_revised_order/test_pop_seq.hdf5\") as f:\n",
    "    truth = f['target'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison functions\n",
    "\n",
    "def get_diffmat(mat1, mat2):\n",
    "    \n",
    "    diffmat = mat1 - mat2\n",
    "    abs_diffmat = np.abs(diffmat)\n",
    "\n",
    "    colwise_maxes1 = np.max(mat1, axis=0)\n",
    "    colwise_maxes2 = np.max(mat2, axis=0)\n",
    "\n",
    "    colwise_maxes_maxes = np.maximum(colwise_maxes1, colwise_maxes2)\n",
    "\n",
    "    relmax3_diffmat = diffmat / colwise_maxes_maxes\n",
    "    relmax3_diffmat = np.abs(relmax3_diffmat)\n",
    "\n",
    "    return relmax3_diffmat\n",
    "\n",
    "\n",
    "def get_summary(arr):\n",
    "    summary = {\n",
    "        \"mean\": np.mean(arr),\n",
    "        \"median\": np.median(arr),\n",
    "        \"minimum\": np.min(arr),\n",
    "        \"maximum\": np.max(arr),\n",
    "        \"q1\": np.percentile(arr, 25),\n",
    "        \"q3\": np.percentile(arr, 75),\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "\n",
    "def plot_hist(arr, bin_num, xlab='Value', ylab='Frequency', title='Histogram'):\n",
    "    plt.hist(arr, bins=bin_num)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting diffmats\n",
    "\n",
    "diff_forward = get_diffmat(forward, truth)\n",
    "diff_reverse = get_diffmat(reverse, truth)\n",
    "diff_forrev = get_diffmat(forrev, truth)\n",
    "diff_forrevshift = get_diffmat(forrevshift, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting summaries\n",
    "\n",
    "get_summary(diff_forward), get_summary(diff_reverse), get_summary(diff_forrev), get_summary(diff_forrevshift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the normal forward matrix had the smallest median and mean of differences between it and the ground truth matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also compared matrices by their column correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corvec_forward = np.empty(5313)\n",
    "corvec_reverse = np.empty(5313)\n",
    "corvec_forrev = np.empty(5313)\n",
    "corvec_forrevshift = np.empty(5313)\n",
    "\n",
    "for col in range(5313):\n",
    "    col_correlation = np.corrcoef(forward[:, col], truth[:, col])[0, 1]\n",
    "    col_correlation = np.corrcoef(reverse[:, col], truth[:, col])[0, 1]\n",
    "    col_correlation = np.corrcoef(forrev[:, col], truth[:, col])[0, 1]\n",
    "    col_correlation = np.corrcoef(forrevshift[:, col], truth[:, col])[0, 1]\n",
    "\n",
    "    corvec_forward[col] = col_correlation\n",
    "    corvec_reverse [col] = col_correlation\n",
    "    corvec_forrev[col] = col_correlation\n",
    "    corvec_forrevshift[col] = col_correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(corvec_forward, 1000, title=\"forward\")\n",
    "get_summary(corvec_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(corvec_reverse, 1000, title=\"reverse\")\n",
    "get_summary(corvec_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(corvec_forrev, 1000, title=\"forrev\")\n",
    "get_summary(corvec_forrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(corvec_forrevshift, 1000, title=\"forrevshift\")\n",
    "get_summary(corvec_forrevshift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the normal forward matrix was the closest to the ground truth with the highest column-wise correlation median and mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: using just the normal forward prediction of an interval will give you the closest results to the ground truth, so that's what we will be using for our predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
